LLaMA (Large Language Model Meta AI) is a family of open-weight large language models developed by Meta AI. It was designed to be efficient, high-performing, and accessible to researchers. Unlike earlier models that required billions of parameters to achieve strong results, LLaMA focuses on scaling efficiently â€” delivering competitive performance even at smaller sizes compared to GPT-style models. LLaMA models are based on the transformer architecture but include several important optimizations, such as Rotary Positional Embeddings (RoPE), SwiGLU activation functions, RMSNorm instead of LayerNorm, and in later versions, Grouped-Query Attention (GQA) for faster inference.
