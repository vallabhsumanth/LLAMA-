 # LLaMA MODEL

LLaMA (Large Language Model Meta AI) is a family of open-weight large language models developed by Meta AI. It was designed to be efficient, high-performing, and accessible to researchers. Unlike earlier models that required billions of parameters to achieve strong results, LLaMA focuses on scaling efficiently — delivering competitive performance even at smaller sizes compared to GPT-style models. LLaMA models are based on the transformer architecture but include several important optimizations, such as Rotary Positional Embeddings (RoPE), SwiGLU activation functions, RMSNorm instead of LayerNorm, and in later versions, Grouped-Query Attention (GQA) for faster inference.

# Key Points about LLaMA
 Developed by Meta AI → open-weight family of large language models.
 Efficient scaling → performs well even with fewer parameters.
 Architecture features:

# Transformer backbone.
Rotary Positional Embeddings (RoPE).
SwiGLU activation (instead of ReLU/GeLU).
RMSNorm (faster & simpler than LayerNorm).
Grouped-Query Attention (from LLaMA-2 onward).

# Versions:

LLaMA-1 (2023) → 7B, 13B, 33B, 65B models.

LLaMA-2 (2023) → Improved models + LLaMA-2-Chat.

LLaMA-3 (2024) → State-of-the-art open LLMs.
